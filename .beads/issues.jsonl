{"id":"baish-4pk","title":"ask returns 404 for gpt-4o-mini on puck.local","description":"Steps to reproduce:\n1) BAISH_OPENAI_BASE_URL=puck.local, BAISH_MODEL=gpt-4o-mini.\n2) Run: ask 'What time is it'\n\nObserved:\n- ask repeatedly tries both puck.local:80/v1/chat/completions and :8000 then fails with HTTP 404 \"The model `gpt-4o-mini` does not exist.\"\n- Subsequent /models call also repeats the connection attempts and lists only two models:\n  - nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8\n  - modelperm-8785c1c323a23a80\n- The default suggested model gpt-4o-mini is missing/unavailable on the puck endpoint, leaving ask unusable with the documented defaults.\n\nSample output:\n```\njkh@plubbit-\u003e ask 'What time is it'\nbaish: resolving puck.local:80/v1/chat/completions\nbaish: connecting to puck.local:80/v1/chat/completions\nbaish: connecting to puck.local:80/v1/chat/completions\nbaish: connecting to puck.local:80/v1/chat/completions\nbaish: connecting to puck.local:80/v1/chat/completions\nbaish: connecting to puck.local:80/v1/chat/completions\nbaish: connecting to puck.local:80/v1/chat/completions\nbaish: resolving puck.local:8000/v1/chat/completions\nbaish: connecting to puck.local:8000/v1/chat/completions\nbaish: connecting to puck.local:8000/v1/chat/completions\nbaish: connecting to puck.local:8000/v1/chat/completions\nbaish: ask: LLM error (HTTP 404): The model `gpt-4o-mini` does not exist.\nbaish: resolving puck.local:80/v1/models\nbaish: connecting to puck.local:80/v1/models\nbaish: connecting to puck.local:80/v1/models\nbaish: connecting to puck.local:80/v1/models\nbaish: connecting to puck.local:80/v1/models\nbaish: connecting to puck.local:80/v1/models\nbaish: connecting to puck.local:80/v1/models\nbaish: resolving puck.local:8000/v1/models\nbaish: connecting to puck.local:8000/v1/models\nbaish: connecting to puck.local:8000/v1/models\nbaish: connecting to puck.local:8000/v1/models\nbaish: BAISH_MODEL=gpt-4o-mini\nbaish: models reported by the API (2):\n  nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8\n  modelperm-8785c1c323a23a80\n```\n\nExpected:\n- ask should succeed with a working default model or provide clear configuration guidance without repeated connection spam.\n","status":"closed","priority":0,"issue_type":"bug","owner":"jordanhubbard@gmail.com","created_at":"2026-01-22T20:40:24.311007-08:00","created_by":"Jordan Hubbard","updated_at":"2026-01-22T21:26:02.308942-08:00","closed_at":"2026-01-22T21:26:02.308942-08:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"baish-4pk","author":"Jordan Hubbard","text":"Expected interactive behavior: after the user runs ask, print a single line like \"Asking \u003chostname\u003e:\u003cmodelname\u003e \u003cquestion text\u003e\" before making requests. Then either (a) print the answer, or (b) if connections to the host fail, say so plainly, or (c) if the model is unavailable, say so and list the models the endpoint reports. In either failure case, also tell the user which environment variables to set to fix it (e.g., BAISH_OPENAI_BASE_URL, BAISH_MODEL, OPENAI_API_KEY, BAISH_OPENAI_PORT/FAIL_FAST/AUTOEXEC as applicable).\n","created_at":"2026-01-23T04:41:34Z"},{"id":2,"issue_id":"baish-4pk","author":"Jordan Hubbard","text":"Expected interactive behavior: after the user runs ask, print a single line like \"Asking \u003chostname\u003e:\u003cmodelname\u003e \u003cquestion text\u003e\" before making requests. Then either (a) print the answer, or (b) if connections to the host fail, say so plainly, or (c) if the model is unavailable, say so and list the models the endpoint reports. In either failure case, also tell the user which environment variables to set to fix it (e.g., BAISH_OPENAI_BASE_URL, BAISH_MODEL, OPENAI_API_KEY, BAISH_OPENAI_PORT/FAIL_FAST/AUTOEXEC as applicable).\n","created_at":"2026-01-23T04:52:54Z"}]}
